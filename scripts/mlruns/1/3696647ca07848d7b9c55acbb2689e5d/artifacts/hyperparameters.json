{
    "model_layers": [
        {
            "units": 128,
            "activation": "relu",
            "dropout": 0.2
        },
        {
            "units": 64,
            "activation": "relu",
            "dropout": 0.2
        },
        {
            "units": 32,
            "activation": "relu",
            "dropout": 0.2
        },
        {
            "units": 16,
            "activation": "relu"
        }
    ],
    "output_activation": "sigmoid",
    "optimizer": "adam",
    "loss": "binary_crossentropy",
    "metrics": [
        "accuracy"
    ],
    "epochs": 20,
    "batch_size": 64
}